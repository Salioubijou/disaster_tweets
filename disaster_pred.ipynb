{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import model_selection,\\\n",
    "linear_model, naive_bayes, metrics\n",
    "import re\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stm = PorterStemmer()\n",
    "\n",
    "def cleaner(txt):\n",
    "    #Cleaning data\n",
    "    txt = re.sub('<[^>]*>', '', txt)\n",
    "    emots = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                          txt)\n",
    "    txt = (re.sub('[\\W]+', ' ', txt.lower())+' '.join(\n",
    "    emots).replace('-', ''))\n",
    "    #Stemmatization and lemmatization\n",
    "    L = [stm.stem(w) for w in txt.split() if w not in stop]\n",
    "    return ' '.join([Word(mot).lemmatize() for mot in L])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id', 'keyword', 'location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'], df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF 5000 features générées au maximun\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "                token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf.fit(X)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51612381, 0.21437958, 0.05893993, ..., 0.07588667, 0.26761377,\n",
       "       0.31747461])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_training(classifier, train_X, train_y, test_X,\n",
    "                test_y, is_neaural_net=False):\n",
    "    \n",
    "    classifier.fit(train_X, train_y)\n",
    "    y_pred = classifier.predict(test_X)\n",
    "    return {'Accuracy':metrics.accuracy_score(y_true=test_y,\n",
    "                                              y_pred=y_pred),\n",
    "           'Precision':metrics.precision_score(y_true=test_y,\n",
    "                                              y_pred=y_pred),\n",
    "           'Recall':metrics.recall_score(y_true=test_y,\n",
    "                                              y_pred=y_pred),\n",
    "           'F1':metrics.f1_score(y_true=test_y,\n",
    "                                              y_pred=y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Baye : \n",
      " {'Accuracy': 0.8010505581089954, 'Precision': 0.8256029684601113, 'Recall': 0.6804281345565749, 'F1': 0.7460184409052807} \n",
      "\n",
      "Logistic :\n",
      " {'Accuracy': 0.8049901510177282, 'Precision': 0.8426103646833013, 'Recall': 0.6712538226299695, 'F1': 0.7472340425531916} \n",
      "\n",
      "SVM : \n",
      " {'Accuracy': 0.804333552199606, 'Precision': 0.8662551440329218, 'Recall': 0.6437308868501529, 'F1': 0.7385964912280701} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "naivBay = naive_bayes.MultinomialNB(alpha=0.6)\n",
    "lr = linear_model.LogisticRegression(C=1., solver='liblinear',\n",
    "                                    random_state=1,\n",
    "                                     penalty='l2')\n",
    "svm = SVC()\n",
    "\n",
    "predNaivBay = clf_training(naivBay, X_train_tfidf,\n",
    "                          y_train, X_test_tfidf, y_test)\n",
    "\n",
    "predLR = clf_training(lr, X_train_tfidf,\n",
    "                          y_train, X_test_tfidf, y_test)\n",
    "\n",
    "predSVM = clf_training(svm, X_train_tfidf,\n",
    "                          y_train, X_test_tfidf, y_test)\n",
    "\n",
    "print('Naive Baye : \\n', predNaivBay, '\\n')\n",
    "print('Logistic :\\n', predLR, '\\n')\n",
    "print('SVM : \\n', predSVM, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AdaBoost Naive Baye : \n",
      " {'Accuracy': 0.8023637557452397, 'Precision': 0.8168761220825853, 'Recall': 0.6957186544342507, 'F1': 0.7514450867052023} \n",
      "\n",
      " AdaBoost Logistic : \n",
      " {'Accuracy': 0.7715036112934996, 'Precision': 0.84, 'Recall': 0.5779816513761468, 'F1': 0.6847826086956522} \n",
      "\n",
      " AdaBoost SVL : \n",
      " {'Accuracy': 0.7715036112934996, 'Precision': 0.84, 'Recall': 0.5779816513761468, 'F1': 0.6847826086956522} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaNaiv = AdaBoostClassifier(base_estimator=naivBay,\n",
    "                         n_estimators=5000, learning_rate=0.1,\n",
    "                         random_state=1)\n",
    "adaLR = AdaBoostClassifier(base_estimator=lr,\n",
    "                         n_estimators=5000, learning_rate=0.1,\n",
    "                         random_state=1)\n",
    "adaSVM = AdaBoostClassifier(base_estimator=svm,\n",
    "                         n_estimators=5000, learning_rate=0.1,\n",
    "                         random_state=1)\n",
    "\n",
    "predAdaNaiv = clf_training(adaNaiv, X_train_tfidf,\n",
    "                          y_train, X_test_tfidf, y_test)\n",
    "predAdaLR = clf_training(adaLR, X_train_tfidf,\n",
    "                          y_train, X_test_tfidf, y_test)\n",
    "predAdaSVM = clf_training(adaLR, X_train_tfidf,\n",
    "                          y_train, X_test_tfidf, y_test)\n",
    "\n",
    "print(' AdaBoost Naive Baye : \\n', predAdaNaiv, '\\n')\n",
    "print(' AdaBoost Logistic : \\n', predAdaLR, '\\n')\n",
    "print(' AdaBoost SVM : \\n', predAdaSVM, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                               random_state=123,\n",
    "                               learning_method='batch')\n",
    "\n",
    "X_topic = lda.fit_transform(X_train_tfidf)\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "http co migrant drown rescuer\n",
      "Topic 2:\n",
      "co http wildfir fatal attack\n",
      "Topic 3:\n",
      "http co bag bodi suicid\n",
      "Topic 4:\n",
      "co http reddit quarantin content\n",
      "Topic 5:\n",
      "co http fire bomb boy\n",
      "Topic 6:\n",
      "co http disast obama typhoon\n",
      "Topic 7:\n",
      "http co scream im crush\n",
      "Topic 8:\n",
      "co http wreck thunder one\n",
      "Topic 9:\n",
      "http co crash mh370 confirm\n",
      "Topic 10:\n",
      "co http loud trap wind\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print('Topic %d:'% (topic_idx +1))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()\\\n",
    "                   [:-n_top_words-1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=5000,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False, token_pattern='\\\\w{1,}',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('multinomialnb',\n",
       "                 MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe_naivBay = make_pipeline(TfidfVectorizer(analyzer='word',\n",
    "                token_pattern=r'\\w{1,}', max_features=5000),\n",
    "    #StandardScaler(),\n",
    "                          #LatentDirichletAllocation(\n",
    "                           #   n_components=100,\n",
    "                            #   random_state=123,\n",
    "                             #  learning_method='batch'),\n",
    "                          naive_bayes.MultinomialNB(alpha=0.2)\n",
    "                             #LogisticRegression(random_state=1,\n",
    "                              #                 solver='lbfgs')\n",
    "                            )\n",
    "\n",
    "pipe_naivBay.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_naivBay.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.800\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : %.3f' % pipe_naivBay.score(\n",
    "X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.747 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F1 %.3f '% f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores : [0.7816092  0.79967159 0.77175698 0.80788177 0.80131363 0.80131363\n",
      " 0.82101806 0.81444992 0.79802956 0.7865353 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=pipe_naivBay, X=X_train,\n",
    "               y=y_train, cv=10, n_jobs=-1)\n",
    "\n",
    "print('CV accuracy scores : %s' % scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy : 0.798 +/- 0.014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('CV accuracy : %.3f +/- %.3f' % (np.mean(scores),\n",
    "                                      np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_ac...\n",
       "                                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('clf',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'clf__alpha': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False,\n",
    "                       preprocessor=None)\n",
    "\n",
    "param_grid = [{'clf__alpha':np.linspace(0.1, 1, 10)}]\n",
    "\n",
    "nb_tfidf = Pipeline([('vect', tfidf),\n",
    "                    ('clf', MultinomialNB())])\n",
    "\n",
    "gs_nb_tfidf = GridSearchCV(nb_tfidf, param_grid,\n",
    "                          scoring='accuracy', cv=10, verbose=2,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "gs_nb_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.6}\n"
     ]
    }
   ],
   "source": [
    "print(gs_nb_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 : 0.804\n",
      "0.74466268146883\n"
     ]
    }
   ],
   "source": [
    "nb = gs_nb_tfidf.best_estimator_\n",
    "\n",
    "print('F1 : %.3f' % nb.score(X_test, y_test))\n",
    "y_pred = nb.predict(X_test)\n",
    "print(f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', header='infer')\n",
    "\n",
    "test_id = test['id']\n",
    "test = test.drop(['id', 'keyword', 'location'], axis=1)\n",
    "predict_test = [[idx,nb.predict([text])[0]] for idx, \\\n",
    "                text in zip(test_id, test['text'])]\n",
    "\n",
    "predict_test_fram = pd.DataFrame(predict_test,\n",
    "                                 columns=['id', 'target'])\n",
    "\n",
    "predict_test_fram.to_csv('predict_test_fram.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
